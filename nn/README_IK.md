# Inverse Kinematics Neural Network

A neural network-based inverse kinematics solver for a 3-joint planar robotic arm.

## Overview

This implementation provides a learning-based approach to inverse kinematics (IK) that maps end-effector positions to joint angles. Unlike analytical IK solutions, the neural network can handle:

- Multiple solutions (the network learns one consistent solution)
- Fast inference (no iterative solving)
- Potential for learning from real robot data
- Easy extension to more complex kinematics

## Network Architecture

The `InverseKinematicsNetwork` consists of:

**Input**: End-effector position `[x, y]` (optionally with link lengths `[L1, L2, L3]`)

**Architecture**:
- Input layer: 2D position (or 5D with link lengths)
- Hidden layers: [128, 256, 256, 128] with ReLU activation and dropout
- Output layer: 3 joint angles `[θ1, θ2, θ3]` in radians

**Output**: Joint angles that achieve the target position

## Files

- `inverse_kinematics_network.py` - Network definition and forward kinematics
- `train_inverse_kinematics.py` - Training script with multiple loss functions
- `test_inverse_kinematics.py` - Testing and visualization tools
- `README_IK.md` - This documentation

## Quick Start

### 1. Train the Network

Train with default settings (position-based loss):

```bash
python train_inverse_kinematics.py
```

The training script will:
- Generate 50,000 training samples
- Generate 5,000 validation samples
- Train for 100 epochs
- Save checkpoints every 10 epochs
- Save the best model based on validation loss

### 2. Test the Trained Model

Test on random targets:

```bash
python test_inverse_kinematics.py --checkpoint checkpoints_ik/best_model.pth --num-tests 10
```

Test a specific target position:

```bash
python test_inverse_kinematics.py --checkpoint checkpoints_ik/best_model.pth --target 1.5 1.0
```

Evaluate performance across the entire workspace:

```bash
python test_inverse_kinematics.py --checkpoint checkpoints_ik/best_model.pth --workspace
```

### 3. Use in Your Code

```python
import torch
from inverse_kinematics_network import InverseKinematicsNetwork

# Load trained model
checkpoint = torch.load('checkpoints_ik/best_model.pth')
model = InverseKinematicsNetwork(link_lengths=(1.0, 1.0))
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Predict joint angles for a target position
target = torch.tensor([[1.5, 1.0]])  # [x, y]
with torch.no_grad():
    joint_angles = model(target)  # [θ1, θ2, θ3]

print(f"Joint angles: {joint_angles}")
```

## Configuration

### Link Lengths

The default configuration uses equal link lengths:
- L1 = 1.0
- L2 = 1.0
- L3 = 1.0
- Total reach = 3.0

To use different link lengths, modify the `CONFIG` in the training script:

```python
CONFIG = {
    'link_lengths': (1.5, 1.0, 0.5),  # Custom lengths
    ...
}
```

### Loss Functions

Three loss types are available:

1. **Position Loss** (default): Minimizes end-effector position error
   - Learns any valid IK solution
   - Best for accuracy at the end-effector

2. **Angle Loss**: Minimizes joint angle error against ground truth
   - Learns to match specific joint configurations
   - Useful when joint limits or preferences matter

3. **Combined Loss**: Weighted combination of both
   - Balances position accuracy and joint consistency

Change in training script:

```python
CONFIG = {
    'loss_type': 'position',  # or 'angle' or 'combined'
    ...
}
```

### Network Architecture

Customize the hidden layer dimensions:

```python
model = InverseKinematicsNetwork(
    link_lengths=(1.0, 1.0),
    hidden_dims=[64, 128, 128, 64]  # Smaller network
)
```

## Training Details

### Dataset Generation

Training data is generated by:
1. Randomly sampling joint angles from [-π, π]
2. Computing forward kinematics to get end-effector positions
3. Using positions as input and angles as ground truth

This ensures all training targets are reachable.

### Training Parameters

Default configuration:
- Batch size: 64
- Learning rate: 1e-3 with ReduceLROnPlateau
- Optimizer: AdamW with weight decay 1e-5
- Gradient clipping: max norm 1.0
- Training samples: 50,000
- Validation samples: 5,000
- Epochs: 100

### Expected Performance

With default settings, the network typically achieves:
- Mean position error: < 0.001
- 95th percentile error: < 0.01
- Training time: ~10-20 minutes on GPU

## Forward Kinematics

The `ForwardKinematics` class provides utilities for:

**Computing end-effector position from joint angles:**

```python
from inverse_kinematics_network import ForwardKinematics

joint_angles = torch.tensor([[0.5, 0.3, -0.2]])  # [θ1, θ2, θ3]
link_lengths = (1.0, 1.0, 1.0)

# Get end-effector position
end_effector, joint_positions = ForwardKinematics.compute(
    joint_angles, link_lengths
)

print(f"End-effector: {end_effector}")  # [x, y]
```

**Generating random configurations:**

```python
from inverse_kinematics_network import generate_random_configurations

joint_angles, positions = generate_random_configurations(
    num_samples=1000,
    link_lengths=(1.0, 1.0)
)
```

## Visualization

The test script generates several visualizations:

1. **Random Tests**: Shows ground truth vs predicted configurations
2. **Specific Target**: Detailed view of a single IK solution
3. **Workspace Evaluation**: Error distribution across the reachable workspace

All visualizations are saved to the `test_results/` directory.

## Tensorboard Monitoring

Monitor training progress:

```bash
tensorboard --logdir logs_ik
```

Tracked metrics:
- Training loss
- Validation loss
- Position error
- Individual loss components (for combined loss)
- Learning rate

## Advanced Usage

### Variable Link Lengths

To train a network that can handle different link lengths:

```python
model = InverseKinematicsNetwork(
    link_lengths=(1.0, 1.0),
    input_link_lengths=True  # Enable link length input
)

# During inference
target = torch.tensor([[1.5, 1.0]])
link_lengths = torch.tensor([[1.5, 1.0]])
joint_angles = model(target, link_lengths)
```

### Error Analysis

Compute pose errors:

```python
from inverse_kinematics_network import compute_pose_error

predicted_angles = model(target_poses)
position_errors, rotation_errors = compute_pose_error(
    predicted_angles,
    target_poses,  # Shape: (B, 3) - [x, y, φ]
    link_lengths=(1.0, 1.0)
)
```

## Limitations

1. **Single Solution**: Networks learn one solution even when multiple valid solutions exist
2. **Training Distribution**: Performance depends on training data distribution
3. **Workspace Boundaries**: May have higher errors near workspace limits
4. **Planar Only**: Current implementation is for 2D planar arms

## Extension Ideas

- Add joint limits to training data
- Include obstacle avoidance in loss function
- Extend to 3D spatial manipulators
- Add velocity/acceleration constraints
- Learn from real robot demonstration data
- Multi-solution prediction with mixture density networks

## Mathematical Background

### Forward Kinematics

For a 3-joint planar arm:

```
x = L1*cos(θ1) + L2*cos(θ1+θ2) + L3*cos(θ1+θ2+θ3)
y = L1*sin(θ1) + L2*sin(θ1+θ2) + L3*sin(θ1+θ2+θ3)
```

### Inverse Kinematics (Neural Network)

The network learns the mapping:

```
[θ1, θ2, θ3] = f_θ(x, y)
```

where `f_θ` is the neural network with parameters `θ`.

## Troubleshooting

**High training loss:**
- Increase network capacity (more/larger hidden layers)
- Increase training samples
- Adjust learning rate
- Try different loss function

**Poor generalization:**
- Increase validation set size
- Add more dropout
- Reduce network size
- Check for overfitting

**Slow training:**
- Reduce batch size
- Use GPU if available
- Decrease network size
- Reduce training samples

## References

For analytical IK solutions and comparison:
- Craig, J.J. "Introduction to Robotics: Mechanics and Control"
- Siciliano, B. "Robotics: Modelling, Planning and Control"

For neural network-based IK:
- Csiszar, A. et al. "Neural Network based Inverse Kinematics Solution" (2017)
- Kinga, M. et al. "Deep Learning for Inverse Kinematics" (2019)
